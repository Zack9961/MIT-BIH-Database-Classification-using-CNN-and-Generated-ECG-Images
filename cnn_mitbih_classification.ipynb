{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a05e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function.\n",
    "def train(epoch, model, loader, criterion, optimizer, device='cpu'):\n",
    "    l = 0\n",
    "    for data in tqdm(loader, desc=f'Epoch {epoch+1:03d}'):\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].squeeze().to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        l += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return l\n",
    "\n",
    "# Test function.\n",
    "def test(model, loader, criterion, device='cpu'):\n",
    "    l = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "            out = model(x)\n",
    "            l += criterion(out, y)\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            y_true += y.tolist()\n",
    "            y_pred += pred.tolist()\n",
    "    return l, correct / total, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Mounta il drive di Google\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definisci le cartelle delle immagini\n",
    "train_dir = '/content/drive/MyDrive/UNI/Magistrale/Applicazioni_IA/Progetto/Images/train'\n",
    "test_dir = '/content/drive/MyDrive/UNI/Magistrale/Applicazioni_IA/Progetto/Images/test'\n",
    "\n",
    "# Definisci le etichette\n",
    "etichette = ['N', 'A', 'V']\n",
    "\n",
    "# Crea un dizionario per mappare le etichette ai valori numerici\n",
    "etichette_map = {etichetta: i for i, etichetta in enumerate(etichette)}\n",
    "\n",
    "# Funzione per caricare le immagini e le loro etichette\n",
    "class ImmaginiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, transform=None):\n",
    "        self.dir = dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        for etichetta in etichette:\n",
    "            path = os.path.join(dir, etichetta)\n",
    "            for file in os.listdir(path):\n",
    "                self.images.append((os.path.join(path, file), etichetta))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, etichetta = self.images[index]\n",
    "        #immagine = torchvision.io.read_image(path, mode=torchvision.io.ImageReadMode.LZW)\n",
    "        immagine = Image.open(path).convert('L') # Converti in bianco e nero\n",
    "        if self.transform:\n",
    "            immagine = self.transform(immagine)\n",
    "        return immagine, etichette_map[etichetta]\n",
    "\n",
    "# Crea il dataset per l'addestramento\n",
    "train_dataset = ImmaginiDataset(train_dir)\n",
    "\n",
    "# Applica le trasformazioni alle immagini\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset.transform = train_transformations\n",
    "\n",
    "# Crea il dataset per il test\n",
    "test_dataset = ImmaginiDataset(test_dir)\n",
    "test_dataset.transform = train_transformations\n",
    "\n",
    "# Crea i dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters.\n",
    "num_classes = len(etichette)\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3)\n",
    "        )\n",
    "        self.fc = nn.Linear(23 * 23 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Create the model.\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and test.\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(epoch, model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, y_true, y_pred = test(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1:03d}: training loss {train_loss:.4f}, test loss {test_loss:.4f}, test acc {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74491939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
